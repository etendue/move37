{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network implementation\n",
    "\n",
    "This notebook shamelessly demands you to implement a DQN - an approximate q-learning algorithm with experience replay and target networks - and see if it works any better this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0effc18e48>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "img = env.render(mode='rgb_array')\n",
    "env.close()\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, state_shape, n_actions,model, epsilon=0):\n",
    "        \"\"\"A simple DQN agent\"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape= state_shape\n",
    "        self.model = model\n",
    "\n",
    "    def get_qvalues(self, state):\n",
    "        \"\"\"\n",
    "        state is observation from gym environment\n",
    "        pytorch needs [batch,s], so add a batch dim in state\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        s = torch.from_numpy(np.expand_dims(state,axis=0)).to(device)\n",
    "        with torch.no_grad():\n",
    "            q = self.model.forward(s)\n",
    "        return np.squeeze(q.cpu().data.numpy(),axis=0)\n",
    "    \n",
    "    def get_state_action(self, state):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        action = self.get_qvalues(state).argmax(axis=-1)\n",
    "        if self.epsilon > 0. and random.random()<eps:\n",
    "            action = np.random.choice(n_actions)\n",
    "            \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network\n",
    "\n",
    "We now need to build a neural network that can map images to state q-values. This network will be called on every agent's step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLModel(nn.Module):\n",
    "    def __init__(self,input_size,output_size,n_l1 = 64, n_l2 = 64):\n",
    "        super(MLModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layer1 = nn.Linear(input_size,n_l1)\n",
    "        self.layer2 = nn.Linear(n_l1,n_l2)\n",
    "        self.output = nn.Linear(n_l2,output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.output(x)\n",
    "    \n",
    "    @property\n",
    "    def genome(self):\n",
    "        return torch.cat((self.layer1.weight.view(-1),self.layer1.bias,\\\n",
    "                                 self.layer2.weight.view(-1),self.layer2.bias,\\\n",
    "                                self.output.weight.view(-1),self.output.bias))\n",
    "    \n",
    "    def update_genome(self,genes):\n",
    "        #calculate index of genes\n",
    "        w1 = self.layer1.weight.numel()\n",
    "        b1 = self.layer1.bias.numel() +w1\n",
    "        w2 = self.layer2.weight.numel() +b1\n",
    "        b2 = self.layer2.bias.numel() + w2\n",
    "        w3 = self.output.weight.numel()+b2\n",
    "        b3 = self.output.bias.numel() +w3\n",
    "        \n",
    "        self.layer1.weight.data = genes[:w1].view(self.layer1.weight.shape)\n",
    "        self.layer1.bias.data = genes[w1:b1].view(self.layer1.bias.shape)\n",
    "        self.layer2.weight.data = genes[b1:w2].view(self.layer2.weight.shape)\n",
    "        self.layer2.bias.data = genes[w2:b2].view(self.layer2.bias.shape)\n",
    "        self.output.weight.data = genes[b2:w3].view(self.output.weight.shape)\n",
    "        self.output.bias.data = genes[w3:b3].view(self.output.bias.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct train & evaluation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our agent to see if it raises any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate(env, agent, n_games=1, greedy=True,render=False):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    t_max = env.spec.timestep_limit or 10000\n",
    "    rewards = []\n",
    "    old_epsilon = agent.epsilon\n",
    "    if greedy:\n",
    "        agent.epsilon = 0.0\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            action = agent.get_state_action(s)\n",
    "            s, r, done, _ = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep (50.0 / 1000.0);\n",
    "                \n",
    "            reward += r\n",
    "            if done: break\n",
    "                \n",
    "        rewards.append(reward)\n",
    "    \n",
    "    if render:\n",
    "        env.close()\n",
    "    \n",
    "    agent.epsilon = old_epsilon\n",
    "    \n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "#### The interface:\n",
    "* `exp_replay.push(state, action, reward, next_state, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward','next_state','done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def head(self):\n",
    "        return self.memory[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with... Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{policy_\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "With Q-reference defined as\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Where\n",
    "* $Q_{target}(s',a')$ denotes q-value of next state and next action predicted by __target_network__\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_loss(target_net,policy_net,batch,device,weights=None,GAMMA = 0.99):\n",
    "    \n",
    "    s_ = torch.as_tensor(batch.state,dtype=torch.float32).to(device)\n",
    "    a_ = torch.as_tensor(batch.action,dtype=torch.int64).to(device)\n",
    "    r_ = torch.as_tensor(batch.reward,dtype=torch.float32).to(device)\n",
    "    ns_= torch.as_tensor(batch.next_state,dtype=torch.float32).to(device)\n",
    "    d_= torch.as_tensor(batch.done,dtype=torch.float32).to(device)\n",
    "    not_d_ = 1 - d_\n",
    "    \n",
    "    #predict Q values\n",
    "    predict_q_ = policy_net(s_).gather(dim=1,index=a_.view(-1,1)).squeeze()\n",
    "    \n",
    "    #predict Next QValues with target network\n",
    "    with torch.no_grad():\n",
    "        predict_next_q_ = target_net(ns_)\n",
    "        predict_v_ = torch.max(predict_next_q_,dim=1)[0] * not_d_\n",
    "        reference_q_ = r_ + GAMMA * predict_v_\n",
    "    \n",
    "    #loss = F.mse_loss(predict_q_,reference_q_)\n",
    "    if weights:\n",
    "        weights_t = torch.as_tensor(weights,dtype=torch.float32).to(device).detach()\n",
    "        loss = ((predict_q_ - reference_q_)**2 *weights_t).mean()\n",
    "    else:\n",
    "        loss = F.smooth_l1_loss(predict_q_,reference_q_)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop\n",
    "\n",
    "It's time to put everything together and see if it learns anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_dim,n_actions = env.observation_space.shape[0],env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = MLModel(state_dim,n_actions).to(device)\n",
    "agent = DQNAgent(state_dim, n_actions,policy_net, epsilon=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-456.6116931812651"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(env, agent, n_games=3,render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy_net = MLModel(state_dim,n_actions,n_l1=64,n_l2=128).to(device)\n",
    "target_net = MLModel(state_dim,n_actions,n_l1=64,n_l2=128).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "agent = DQNAgent(state_dim, n_actions,policy_net, epsilon=0.5)\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "td_loss_m_history = []\n",
    "BATCH_SIZE=64\n",
    "TIME_STEPS=10**5*4\n",
    "LEARNING_RATE= 0.0005\n",
    "EPSILON_START = 0.5\n",
    "EPSILON_END = 0.001\n",
    "agent.epsilon=EPSILON_START\n",
    "exp_replay = ReplayMemory(10**5)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(),lr= LEARNING_RATE)\n",
    "\n",
    "# for training\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# for evaluation\n",
    "eval_env = gym.make(\"LunarLander-v2\")\n",
    "s = env.reset()\n",
    "for i in trange(TIME_STEPS):\n",
    "    \n",
    "    # play\n",
    "    a = agent.get_state_action(s)\n",
    "    s_next,r,done, _ = env.step(a)\n",
    "    \n",
    "    transition = Transition(s,a,r,s_next,done)\n",
    "    exp_replay.push(transition)\n",
    "    \n",
    "    s = s_next\n",
    "    \n",
    "    if done:\n",
    "        s = env.reset()\n",
    "    \n",
    "    if len(exp_replay) < BATCH_SIZE:\n",
    "        continue\n",
    "    \n",
    "    # train\n",
    "    # < sample data from experience replay>\n",
    "    transitions = exp_replay.sample(BATCH_SIZE-1)\n",
    "    transitions.append(transition)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "   \n",
    "    # loss = < compute TD loss >\n",
    "    loss = td_loss(target_net,policy_net,batch,device)\n",
    "    \n",
    "    #< minimize loss by gradient descent >\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    td_loss_history.append(loss.cpu().data.numpy())\n",
    "    \n",
    "    count = min(len(td_loss_history),100)\n",
    "    td_loss_m_history.append(np.mean(td_loss_history[-count:]))\n",
    "    \n",
    "    # adjust agent parameters\n",
    "    agent.epsilon -= (EPSILON_START - EPSILON_END)/TIME_STEPS\n",
    "    #Load agent weights into target_network\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        \n",
    "        mean_rw_history.append(evaluate(eval_env, agent, n_games=5))\n",
    "        if mean_rw_history[-1] > 200:\n",
    "            break\n",
    "        \n",
    "        clear_output(True)\n",
    "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "        plt.figure(figsize=[12, 4])        \n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title(\"mean reward per game\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title(\"TD loss history (moving average)\")\n",
    "        plt.plot(td_loss_m_history)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_net.state_dict(), \"checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.load_state_dict(torch.load(\"checkpoint.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-75f81927fbe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_env' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate(eval_env, agent, n_games=1,render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Replay Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from: \n",
    "    https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
    "    Store the data with its priority in tree and data frameworks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # for all priority values\n",
    "        self.data_pointer = 0\n",
    "        self.length= 0\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        # [--------------parent nodes-------------][-------leaves to record priority-------]\n",
    "        #             size: capacity - 1                       size: capacity\n",
    "        self.data = np.zeros(capacity, dtype=object)  # for all transitions\n",
    "        # [--------------data frame-------------]\n",
    "        #             size: capacity\n",
    "\n",
    "    def add(self, p, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data  # update data_frame\n",
    "        self.update(tree_idx, p)  # update tree_frame\n",
    "\n",
    "        self.data_pointer += 1\n",
    "        if self.length< self.capacity:\n",
    "            self.length +=1\n",
    "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
    "            self.data_pointer = 0\n",
    "\n",
    "    def update(self, tree_idx, p):\n",
    "        change = p - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = p\n",
    "        # then propagate the change through tree\n",
    "        while tree_idx != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for transitions\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_idx = 0\n",
    "        while True:     # the while loop is faster than the method in the reference code\n",
    "            cl_idx = 2 * parent_idx + 1         # this leaf's left and right kids\n",
    "            cr_idx = cl_idx + 1\n",
    "            if cl_idx >= len(self.tree):        # reach bottom, end search\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:       # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[cl_idx] or self.tree[cr_idx] == 0.0:\n",
    "                    parent_idx = cl_idx\n",
    "                else:\n",
    "                    v -= self.tree[cl_idx]\n",
    "                    parent_idx = cr_idx\n",
    "\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_p(self):\n",
    "        return self.tree[0]  # the root\n",
    "    \n",
    "    @property\n",
    "    def leaves(self):\n",
    "        return self.tree[-self.capacity:] # leaves\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "class PriorExpReplay(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    epsilon = 0.01  # small amount to avoid zero priority\n",
    "    alpha = 0.6  # [0~1] convert the importance of TD error to priority\n",
    "    beta = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    beta_increment_per_sampling = 0.0001\n",
    "    abs_err_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def push(self, transition,td_error):\n",
    "        #max_p = max(np.max(self.tree.leaves),self.abs_err_upper)\n",
    "        p = np.power(td_error,self.alpha)\n",
    "        self.tree.add(p, transition)   # set the max p for new p\n",
    "\n",
    "    def sample(self, n):\n",
    "        indice = []\n",
    "        transitions = []\n",
    "        weights = []\n",
    "        \n",
    "        pri_seg = self.tree.total_p / float(n)       # priority segment\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        #min_prob = np.min(self.tree.leaves[self.tree.leaves.nonzero()]) / self.tree.total_p # for later calculate ISweight\n",
    "        p_avg = self.tree.total_p/len(self.tree)\n",
    "        \n",
    "        for i in range(n):\n",
    "            a, b = pri_seg * i, pri_seg * (i + 1)\n",
    "            v = np.random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get_leaf(v)\n",
    "            \n",
    "            indice.append(idx)\n",
    "            transitions.append(data)\n",
    "            weights.append((p/p_avg)** (-self.beta))\n",
    "        return indice, transitions, weights\n",
    "\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        #abs_errors += self.epsilon  # convert to abs and avoid 0\n",
    "        #clipped_errors = np.minimum(abs_errors, self.abs_err_upper)\n",
    "        ps = np.power(abs_errors, self.alpha)\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 66147/400000 [17:40<1:23:34, 66.57it/s]"
     ]
    }
   ],
   "source": [
    "def td_error(target_net,policy_net,batch,device,GAMMA = 0.99):\n",
    "    s_ = torch.as_tensor(batch.state,dtype=torch.float32).to(device)\n",
    "    a_ = torch.as_tensor(batch.action,dtype=torch.int64).to(device)\n",
    "    r_ = torch.as_tensor(batch.reward,dtype=torch.float32).to(device)\n",
    "    ns_= torch.as_tensor(batch.next_state,dtype=torch.float32).to(device)\n",
    "    d_= torch.as_tensor(batch.done,dtype=torch.float32).to(device)\n",
    "    not_d_ = 1 - d_\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #predict Q values\n",
    "        predict_q_ = policy_net(s_).gather(dim=1,index=a_.view(-1,1)).squeeze()\n",
    "        #predict Next QValues with target network\n",
    "        predict_next_q_ = target_net(ns_)\n",
    "        predict_v_ = torch.max(predict_next_q_,dim=1)[0] * not_d_\n",
    "        reference_q_ = r_ + GAMMA * predict_v_\n",
    "    \n",
    "        #loss = F.mse_loss(predict_q_,reference_q_)\n",
    "        error = torch.abs(predict_q_- reference_q_)\n",
    "    \n",
    "    return error.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = MLModel(state_dim,n_actions,n_l1=64,n_l2=128).to(device)\n",
    "target_net = MLModel(state_dim,n_actions,n_l1=64,n_l2=128).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "agent = DQNAgent(state_dim, n_actions,policy_net, epsilon=0.5)\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "td_loss_m_history = []\n",
    "BATCH_SIZE=64\n",
    "TIME_STEPS=10**5*4\n",
    "LEARNING_RATE= 0.0005\n",
    "EPSILON_START = 0.5\n",
    "EPSILON_END = 0.001\n",
    "agent.epsilon=EPSILON_START\n",
    "exp_replay = PriorExpReplay(10**5)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(),lr= LEARNING_RATE)\n",
    "\n",
    "# for training\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# for evaluation\n",
    "eval_env = gym.make(\"LunarLander-v2\")\n",
    "s = env.reset()\n",
    "for i in trange(TIME_STEPS):\n",
    "    \n",
    "    # play\n",
    "    a = agent.get_state_action(s)\n",
    "    s_next,r,done, _ = env.step(a)\n",
    "    \n",
    "    transition = Transition(s,a,r,s_next,done)\n",
    "    s = s_next\n",
    "    \n",
    "    if done:\n",
    "        s = env.reset()\n",
    "    \n",
    "    batch = Transition(*zip(*[transition]))\n",
    "    td_err = td_error(target_net,policy_net,batch,device)\n",
    "    exp_replay.push(transition,td_err)\n",
    "    if i < BATCH_SIZE:\n",
    "        continue\n",
    "    \n",
    "    # train\n",
    "    # < sample data from experience replay>\n",
    "    indice,transitions,weights = exp_replay.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "   \n",
    "    # loss = < compute TD loss >\n",
    "    loss = td_loss(target_net,policy_net,batch,device,weights)\n",
    "    \n",
    "    #< minimize loss by gradient descent >\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    td_errs = td_error(target_net,policy_net,batch,device)\n",
    "    exp_replay.batch_update(indice,td_errs)\n",
    "    \n",
    "    td_loss_history.append(loss.cpu().data.numpy())\n",
    "    \n",
    "    count = min(len(td_loss_history),100)\n",
    "    td_loss_m_history.append(np.mean(td_loss_history[-count:]))\n",
    "    \n",
    "    # adjust agent parameters\n",
    "    agent.epsilon -= (EPSILON_START - EPSILON_END)/TIME_STEPS\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        #Load agent weights into target_network\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        \n",
    "        mean_rw_history.append(evaluate(eval_env, agent, n_games=5))\n",
    "        \n",
    "        if mean_rw_history[-1] > 200:\n",
    "            break\n",
    "        \n",
    "        clear_output(True)\n",
    "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "        plt.figure(figsize=[12, 4])        \n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title(\"mean reward per game\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title(\"TD loss history (moving average)\")\n",
    "        plt.plot(td_loss_m_history)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon=0 # Don't forget to reset epsilon back to previous value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env_monitor = gym.wrappers.Monitor(eval_env,directory=\"videos\",force=True)\n",
    "sessions = [evaluate(env_monitor, agent, n_games=1,render=True) for _ in range(1)]\n",
    "env_monitor.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.1.25718.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus I (2+ pts)\n",
    "\n",
    "Implement and train double q-learning.\n",
    "\n",
    "This task contains of\n",
    "* Implementing __double q-learning__ or __dueling q-learning__ or both (see tips below)\n",
    "* Training a network till convergence\n",
    "  * Full points will be awarded if your network gets average score of >=10 (see \"evaluating results\")\n",
    "  * Higher score = more points as usual\n",
    "  * If you're running out of time, it's okay to submit a solution that hasn't converged yet and updating it when it converges. _Lateness penalty will not increase for second submission_, so submitting first one in time gets you no penalty.\n",
    "\n",
    "\n",
    "#### Tips:\n",
    "* Implementing __double q-learning__ shouldn't be a problem if you've already have target networks in place.\n",
    "  * You will probably need `tf.argmax` to select best actions\n",
    "  * Here's an original [article](https://arxiv.org/abs/1509.06461)\n",
    "\n",
    "* __Dueling__ architecture is also quite straightforward if you have standard DQN.\n",
    "  * You will need to change network architecture, namely the q-values layer\n",
    "  * It must now contain two heads: V(s) and A(s,a), both dense layers\n",
    "  * You should then add them up via elemwise sum layer.\n",
    "  * Here's an [article](https://arxiv.org/pdf/1511.06581.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus II (5+ pts): Prioritized experience replay\n",
    "\n",
    "In this section, you're invited to implement prioritized experience replay\n",
    "\n",
    "* You will probably need to provide a custom data structure\n",
    "* Once pool.update is called, collect the pool.experience_replay.observations, actions, rewards and is_alive and store them in your data structure\n",
    "* You can now sample such transitions in proportion to the error (see [article](https://arxiv.org/abs/1511.05952)) for training.\n",
    "\n",
    "It's probably more convenient to explicitly declare inputs for \"sample observations\", \"sample actions\" and so on to plug them into q-learning.\n",
    "\n",
    "Prioritized (and even normal) experience replay should greatly reduce amount of game sessions you need to play in order to achieve good performance. \n",
    "\n",
    "While it's effect on runtime is limited for atari, more complicated envs (further in the course) will certainly benefit for it.\n",
    "\n",
    "Prioritized experience replay only supports off-policy algorithms, so pls enforce `n_steps=1` in your q-learning reference computation (default is 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuroevolution Search method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_initialization(size,n_input,n_output):\n",
    "    return [MLModel(n_input,n_output) for _ in range(size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitness_score(model,agent,env):\n",
    "    agent.model = model\n",
    "    return evaluate(env, agent, n_games=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breed(parent1,parent2,mutation_rate=0.1):\n",
    "    gen1 = parent1.genome\n",
    "    gen2 = parent2.genome\n",
    "    gen_length= gen1.numel()\n",
    "    genes_pool = torch.stack((gen1,gen2),dim=1)\n",
    "    #cross_over\n",
    "    child_genes = genes_pool.gather(1, index=torch.randint(0,2,size=(gen_length,1),dtype=torch.int64))\n",
    "    child_genes.squeeze_()\n",
    "    #mutation\n",
    "    if mutation_rate>0:\n",
    "        mutated_num = int(gen_length * mutation_rate)\n",
    "        mutated_idx = torch.randint(gen_length,(mutated_num,),dtype=torch.int64)\n",
    "        mutation_vals = torch.rand((mutated_num,))-0.5\n",
    "        child_genes.index_add_(0,mutated_idx,mutation_vals)\n",
    "        \n",
    "        \n",
    "    child = MLModel(parent2.input_size,parent2.output_size)\n",
    "    child.update_genome(child_genes)\n",
    "    \n",
    "    return child "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve(populations,fitness_scores,agent,env, survive_rate = 0.2,epsilon = 0.001):\n",
    "    # test fitness\n",
    "    pop_length= len(populations)\n",
    "    \n",
    "    # convert fitness_scores to probablity for survive\n",
    "    scores= np.array(fitness_scores)\n",
    "    min_score = np.min(scores,axis=0)\n",
    "    scores = scores- min_score + epsilon\n",
    "    probablities = (scores/ np.linalg.norm(scores))**2\n",
    "    \n",
    "    #select survivors\n",
    "    survive_count = int(pop_length * survive_rate)\n",
    "    survive_idx = np.random.choice(pop_length,size=survive_count,p = probablities)\n",
    "    survivors = np.array(populations)[survive_idx].tolist()\n",
    "    \n",
    "    #breed\n",
    "    child_count = pop_length - survive_count\n",
    "    children = []\n",
    "    \n",
    "    for _ in range(child_count):\n",
    "        p1,p2 = np.random.choice(survive_count,size=2)\n",
    "        parent1 = survivors[p1]\n",
    "        parent2 = survivors[p2]\n",
    "        child = breed(parent1,parent2)\n",
    "        children.append(child)\n",
    "        \n",
    "    new_pops = survivors + children\n",
    "    fitness_scores = [get_fitness_score(model,agent,env) for model in new_pops]\n",
    "    \n",
    "    return new_pops,fitness_scores\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Episode 0  Top 3 Scores: -118.47, -118.60, -119.53\n",
      "Episode 100  Top 3 Scores: -40.22, -60.17, -69.29\n",
      "Episode 200  Top 3 Scores: 35.07, 2.21, -11.07926\n",
      "Episode 228  Top 3 Scores: 80.58, 34.57, 18.40evolution reaches target species with score:-455.0472179583442\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from collections import deque\n",
    "POP_SIZE=50\n",
    "GENERATION_NUM = 1000\n",
    "print('training...')\n",
    "# initialize polulation\n",
    "pops = population_initialization(POP_SIZE,state_dim,n_actions)\n",
    "agent = DQNAgent(state_dim,n_actions,pops[0],epsilon=0)\n",
    "evolve_env = gym.make(\"LunarLander-v2\")\n",
    "evolve_env.seed(0)\n",
    "score_history1 = deque(maxlen=50)\n",
    "score_history2 = deque(maxlen=50)\n",
    "score_history3 = deque(maxlen=50)\n",
    "\n",
    "fitness_scores = [get_fitness_score(model,agent,env) for model in pops]\n",
    "best_pop = pops[0]\n",
    "\n",
    "for i in range(GENERATION_NUM):\n",
    "    pops, fitness_scores = evolve(pops,fitness_scores,agent,evolve_env)\n",
    "    score_sort_idx = np.argsort(fitness_scores)\n",
    "\n",
    "    score_history1.append(fitness_scores[score_sort_idx[-1]])\n",
    "    score_history2.append(fitness_scores[score_sort_idx[-2]])\n",
    "    score_history3.append(fitness_scores[score_sort_idx[-3]])\n",
    "    print('\\rEpisode {}  Top 3 Scores: {:.2f}, {:.2f}, {:.2f}'.format(i, \\\n",
    "                    np.mean(score_history1),np.mean(score_history2),np.mean(score_history3)),end=\"\")\n",
    "    \n",
    "    if i %100 == 0:\n",
    "        print('\\rEpisode {}  Top 3 Scores: {:.2f}, {:.2f}, {:.2f}'.format(i, \\\n",
    "                    np.mean(score_history1),np.mean(score_history2),np.mean(score_history3)))\n",
    "    \n",
    "    if np.mean(score_history1) >= 200:\n",
    "        print(\"\\nEvolution reaches target species with score:{}\".format(fitness_scores[score_sort_idx[-1]]))\n",
    "        best_pop = pops[score_sort_idx[-1]]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265.8409383010742"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model = best_pop\n",
    "evaluate(evolve_env, agent, n_games=1,render=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
