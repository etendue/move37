{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network implementation\n",
    "\n",
    "This notebook shamelessly demands you to implement a DQN - an approximate q-learning algorithm with experience replay and target networks - and see if it works any better this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "img = env.render(mode='rgb_array')\n",
    "env.close()\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, state_shape, n_actions,model, epsilon=0):\n",
    "        \"\"\"A simple DQN agent\"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape= state_shape\n",
    "        self.model = model\n",
    "\n",
    "    def get_qvalues(self, state):\n",
    "        \"\"\"\n",
    "        state is observation from gym environment\n",
    "        pytorch needs [batch,s], so add a batch dim in state\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        s = torch.from_numpy(np.expand_dims(state,axis=0)).to(device)\n",
    "        with torch.no_grad():\n",
    "            q = self.model.forward(s)\n",
    "        return np.squeeze(q.cpu().data.numpy(),axis=0)\n",
    "    \n",
    "    def get_state_action(self, state):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        action = self.get_qvalues(state).argmax(axis=-1)\n",
    "        if self.epsilon > 0. and random.random()<eps:\n",
    "            action = np.random.choice(n_actions)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        \"\"\"interface for Agent\"\"\"\n",
    "        return get_state_action(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network\n",
    "\n",
    "We now need to build a neural network that can map images to state q-values. This network will be called on every agent's step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLModel(nn.Module):\n",
    "    def __init__(self,input_size,output_size,n_l1 = 64, n_l2 = 64):\n",
    "        super(MLModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layer1 = nn.Linear(input_size,n_l1)\n",
    "        self.layer2 = nn.Linear(n_l1,n_l2)\n",
    "        self.output = nn.Linear(n_l2,output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.output(x)\n",
    "    \n",
    "    @property\n",
    "    def genome(self):\n",
    "        return torch.cat((self.layer1.weight.view(-1),self.layer1.bias,\\\n",
    "                                 self.layer2.weight.view(-1),self.layer2.bias,\\\n",
    "                                self.output.weight.view(-1),self.output.bias))\n",
    "    \n",
    "    def update_genome(self,genes):\n",
    "        #calculate index of genes\n",
    "        w1 = self.layer1.weight.numel()\n",
    "        b1 = self.layer1.bias.numel() +w1\n",
    "        w2 = self.layer2.weight.numel() +b1\n",
    "        b2 = self.layer2.bias.numel() + w2\n",
    "        w3 = self.output.weight.numel()+b2\n",
    "        b3 = self.output.bias.numel() +w3\n",
    "        \n",
    "        self.layer1.weight.data = genes[:w1].view(self.layer1.weight.shape)\n",
    "        self.layer1.bias.data = genes[w1:b1].view(self.layer1.bias.shape)\n",
    "        self.layer2.weight.data = genes[b1:w2].view(self.layer2.weight.shape)\n",
    "        self.layer2.bias.data = genes[w2:b2].view(self.layer2.bias.shape)\n",
    "        self.output.weight.data = genes[b2:w3].view(self.output.weight.shape)\n",
    "        self.output.bias.data = genes[w3:b3].view(self.output.bias.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct train & evaluation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our agent to see if it raises any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate(env, agent, n_games=1,render=False):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    t_max = env.spec.timestep_limit or 10000\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            action = agent.get_action(s)\n",
    "            s, r, done, _ = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep (50.0 / 1000.0);\n",
    "                \n",
    "            reward += r\n",
    "            if done: break\n",
    "                \n",
    "        rewards.append(reward)\n",
    "    \n",
    "    if render:\n",
    "        env.close()\n",
    "    \n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "#### The interface:\n",
    "* `exp_replay.push(state, action, reward, next_state, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward','next_state','done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def head(self):\n",
    "        return self.memory[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with... Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{policy_\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "With Q-reference defined as\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Where\n",
    "* $Q_{target}(s',a')$ denotes q-value of next state and next action predicted by __target_network__\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_loss(target_net,policy_net,batch,device,weights=None,GAMMA = 0.99):\n",
    "    \n",
    "    s_ = torch.as_tensor(batch.state,dtype=torch.float32).to(device)\n",
    "    a_ = torch.as_tensor(batch.action,dtype=torch.int64).to(device)\n",
    "    r_ = torch.as_tensor(batch.reward,dtype=torch.float32).to(device)\n",
    "    ns_= torch.as_tensor(batch.next_state,dtype=torch.float32).to(device)\n",
    "    d_= torch.as_tensor(batch.done,dtype=torch.float32).to(device)\n",
    "    not_d_ = 1 - d_\n",
    "    \n",
    "    #predict Q values\n",
    "    predict_q_ = policy_net(s_).gather(dim=1,index=a_.view(-1,1)).squeeze()\n",
    "    \n",
    "    #predict Next QValues with target network\n",
    "    with torch.no_grad():\n",
    "        predict_next_q_ = target_net(ns_)\n",
    "        predict_v_ = torch.max(predict_next_q_,dim=1)[0] * not_d_\n",
    "        reference_q_ = r_ + GAMMA * predict_v_\n",
    "    \n",
    "    #loss = F.mse_loss(predict_q_,reference_q_)\n",
    "    if weights:\n",
    "        weights_t = torch.as_tensor(weights,dtype=torch.float32).to(device).detach()\n",
    "        loss = ((predict_q_ - reference_q_)**2 *weights_t).mean()\n",
    "    else:\n",
    "        loss = F.smooth_l1_loss(predict_q_,reference_q_)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop\n",
    "\n",
    "It's time to put everything together and see if it learns anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_dim,n_actions = env.observation_space.shape[0],env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = MLModel(state_dim,n_actions).to(device)\n",
    "agent = DQNAgent(state_dim, n_actions,policy_net, epsilon=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(env, agent, n_games=3,render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy_net = MLModel(state_dim,n_actions,n_l1=64,n_l2=128).to(device)\n",
    "target_net = MLModel(state_dim,n_actions,n_l1=64,n_l2=128).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "agent = DQNAgent(state_dim, n_actions,policy_net, epsilon=0.5)\n",
    "test_agent = DQNAgent(state_dim, n_actions,policy_net, epsilon=0.0)\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "td_loss_m_history = []\n",
    "BATCH_SIZE=64\n",
    "TIME_STEPS=10**5*4\n",
    "LEARNING_RATE= 0.0005\n",
    "EPSILON_START = 0.5\n",
    "EPSILON_END = 0.001\n",
    "agent.epsilon=EPSILON_START\n",
    "exp_replay = ReplayMemory(10**5)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(),lr= LEARNING_RATE)\n",
    "\n",
    "# for training\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# for evaluation\n",
    "eval_env = gym.make(\"LunarLander-v2\")\n",
    "s = env.reset()\n",
    "for i in trange(TIME_STEPS):\n",
    "    \n",
    "    # play\n",
    "    a = agent.get_state_action(s)\n",
    "    s_next,r,done, _ = env.step(a)\n",
    "    \n",
    "    transition = Transition(s,a,r,s_next,done)\n",
    "    exp_replay.push(transition)\n",
    "    \n",
    "    s = s_next\n",
    "    \n",
    "    if done:\n",
    "        s = env.reset()\n",
    "    \n",
    "    if len(exp_replay) < BATCH_SIZE:\n",
    "        continue\n",
    "    \n",
    "    # train\n",
    "    # < sample data from experience replay>\n",
    "    transitions = exp_replay.sample(BATCH_SIZE-1)\n",
    "    transitions.append(transition)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "   \n",
    "    # loss = < compute TD loss >\n",
    "    loss = td_loss(target_net,policy_net,batch,device)\n",
    "    \n",
    "    #< minimize loss by gradient descent >\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    td_loss_history.append(loss.cpu().data.numpy())\n",
    "    \n",
    "    count = min(len(td_loss_history),100)\n",
    "    td_loss_m_history.append(np.mean(td_loss_history[-count:]))\n",
    "    \n",
    "    # adjust agent parameters\n",
    "    agent.epsilon -= (EPSILON_START - EPSILON_END)/TIME_STEPS\n",
    "    #Load agent weights into target_network\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        \n",
    "        mean_rw_history.append(evaluate(eval_env, test_agent, n_games=5))\n",
    "        if mean_rw_history[-1] > 200:\n",
    "            break\n",
    "        \n",
    "        clear_output(True)\n",
    "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "        plt.figure(figsize=[12, 4])        \n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title(\"mean reward per game\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title(\"TD loss history (moving average)\")\n",
    "        plt.plot(td_loss_m_history)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_net.state_dict(), \"checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.load_state_dict(torch.load(\"checkpoint.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(eval_env, test_agent, n_games=1,render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Replay Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from: \n",
    "    https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
    "    Store the data with its priority in tree and data frameworks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # for all priority values\n",
    "        self.data_pointer = 0\n",
    "        self.length= 0\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        # [--------------parent nodes-------------][-------leaves to record priority-------]\n",
    "        #             size: capacity - 1                       size: capacity\n",
    "        self.data = np.zeros(capacity, dtype=object)  # for all transitions\n",
    "        # [--------------data frame-------------]\n",
    "        #             size: capacity\n",
    "\n",
    "    def add(self, p, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data  # update data_frame\n",
    "        self.update(tree_idx, p)  # update tree_frame\n",
    "\n",
    "        self.data_pointer += 1\n",
    "        if self.length< self.capacity:\n",
    "            self.length +=1\n",
    "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
    "            self.data_pointer = 0\n",
    "\n",
    "    def update(self, tree_idx, p):\n",
    "        change = p - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = p\n",
    "        # then propagate the change through tree\n",
    "        while tree_idx != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for transitions\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_idx = 0\n",
    "        while True:     # the while loop is faster than the method in the reference code\n",
    "            cl_idx = 2 * parent_idx + 1         # this leaf's left and right kids\n",
    "            cr_idx = cl_idx + 1\n",
    "            if cl_idx >= len(self.tree):        # reach bottom, end search\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:       # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[cl_idx] or self.tree[cr_idx] == 0.0:\n",
    "                    parent_idx = cl_idx\n",
    "                else:\n",
    "                    v -= self.tree[cl_idx]\n",
    "                    parent_idx = cr_idx\n",
    "\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_p(self):\n",
    "        return self.tree[0]  # the root\n",
    "    \n",
    "    @property\n",
    "    def leaves(self):\n",
    "        return self.tree[-self.capacity:] # leaves\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "class PriorExpReplay(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    epsilon = 0.01  # small amount to avoid zero priority\n",
    "    alpha = 0.6  # [0~1] convert the importance of TD error to priority\n",
    "    beta = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    beta_increment_per_sampling = 0.0001\n",
    "    abs_err_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def push(self, transition,td_error):\n",
    "        #max_p = max(np.max(self.tree.leaves),self.abs_err_upper)\n",
    "        p = np.power(td_error,self.alpha)\n",
    "        self.tree.add(p, transition)   # set the max p for new p\n",
    "\n",
    "    def sample(self, n):\n",
    "        indice = []\n",
    "        transitions = []\n",
    "        weights = []\n",
    "        \n",
    "        pri_seg = self.tree.total_p / float(n)       # priority segment\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        #min_prob = np.min(self.tree.leaves[self.tree.leaves.nonzero()]) / self.tree.total_p # for later calculate ISweight\n",
    "        p_avg = self.tree.total_p/len(self.tree)\n",
    "        \n",
    "        for i in range(n):\n",
    "            a, b = pri_seg * i, pri_seg * (i + 1)\n",
    "            v = np.random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get_leaf(v)\n",
    "            \n",
    "            indice.append(idx)\n",
    "            transitions.append(data)\n",
    "            weights.append((p/p_avg)** (-self.beta))\n",
    "        return indice, transitions, weights\n",
    "\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        #abs_errors += self.epsilon  # convert to abs and avoid 0\n",
    "        #clipped_errors = np.minimum(abs_errors, self.abs_err_upper)\n",
    "        ps = np.power(abs_errors, self.alpha)\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_error(target_net,policy_net,batch,device,GAMMA = 0.99):\n",
    "    s_ = torch.as_tensor(batch.state,dtype=torch.float32).to(device)\n",
    "    a_ = torch.as_tensor(batch.action,dtype=torch.int64).to(device)\n",
    "    r_ = torch.as_tensor(batch.reward,dtype=torch.float32).to(device)\n",
    "    ns_= torch.as_tensor(batch.next_state,dtype=torch.float32).to(device)\n",
    "    d_= torch.as_tensor(batch.done,dtype=torch.float32).to(device)\n",
    "    not_d_ = 1 - d_\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #predict Q values\n",
    "        predict_q_ = policy_net(s_).gather(dim=1,index=a_.view(-1,1)).squeeze()\n",
    "        #predict Next QValues with target network\n",
    "        predict_next_q_ = target_net(ns_)\n",
    "        predict_v_ = torch.max(predict_next_q_,dim=1)[0] * not_d_\n",
    "        reference_q_ = r_ + GAMMA * predict_v_\n",
    "    \n",
    "        #loss = F.mse_loss(predict_q_,reference_q_)\n",
    "        error = torch.abs(predict_q_- reference_q_)\n",
    "    \n",
    "    return error.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = MLModel(state_dim,n_actions,n_l1=64,n_l2=128).to(device)\n",
    "target_net = MLModel(state_dim,n_actions,n_l1=64,n_l2=128).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "agent = DQNAgent(state_dim, n_actions,policy_net, epsilon=0.5)\n",
    "test_agent = DQNAgent(state_dim, n_actions,policy_net, epsilon=0.0)\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "td_loss_m_history = []\n",
    "BATCH_SIZE=64\n",
    "TIME_STEPS=10**5*4\n",
    "LEARNING_RATE= 0.0005\n",
    "EPSILON_START = 0.5\n",
    "EPSILON_END = 0.001\n",
    "agent.epsilon=EPSILON_START\n",
    "exp_replay = PriorExpReplay(10**5)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(),lr= LEARNING_RATE)\n",
    "\n",
    "# for training\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# for evaluation\n",
    "eval_env = gym.make(\"LunarLander-v2\")\n",
    "s = env.reset()\n",
    "for i in trange(TIME_STEPS):\n",
    "    \n",
    "    # play\n",
    "    a = agent.get_state_action(s)\n",
    "    s_next,r,done, _ = env.step(a)\n",
    "    \n",
    "    transition = Transition(s,a,r,s_next,done)\n",
    "    s = s_next\n",
    "    \n",
    "    if done:\n",
    "        s = env.reset()\n",
    "    \n",
    "    batch = Transition(*zip(*[transition]))\n",
    "    td_err = td_error(target_net,policy_net,batch,device)\n",
    "    exp_replay.push(transition,td_err)\n",
    "    if i < BATCH_SIZE:\n",
    "        continue\n",
    "    \n",
    "    # train\n",
    "    # < sample data from experience replay>\n",
    "    indice,transitions,weights = exp_replay.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "   \n",
    "    # loss = < compute TD loss >\n",
    "    loss = td_loss(target_net,policy_net,batch,device,weights)\n",
    "    \n",
    "    #< minimize loss by gradient descent >\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    td_errs = td_error(target_net,policy_net,batch,device)\n",
    "    exp_replay.batch_update(indice,td_errs)\n",
    "    \n",
    "    td_loss_history.append(loss.cpu().data.numpy())\n",
    "    \n",
    "    count = min(len(td_loss_history),100)\n",
    "    td_loss_m_history.append(np.mean(td_loss_history[-count:]))\n",
    "    \n",
    "    # adjust agent parameters\n",
    "    agent.epsilon -= (EPSILON_START - EPSILON_END)/TIME_STEPS\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        #Load agent weights into target_network\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        \n",
    "        mean_rw_history.append(evaluate(eval_env, test_agent, n_games=5))\n",
    "        \n",
    "        if mean_rw_history[-1] > 200:\n",
    "            break\n",
    "        \n",
    "        clear_output(True)\n",
    "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "        plt.figure(figsize=[12, 4])        \n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title(\"mean reward per game\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title(\"TD loss history (moving average)\")\n",
    "        plt.plot(td_loss_m_history)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon=0 # Don't forget to reset epsilon back to previous value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env_monitor = gym.wrappers.Monitor(eval_env,directory=\"videos\",force=True)\n",
    "sessions = [evaluate(env_monitor, agent, n_games=1,render=True) for _ in range(1)]\n",
    "env_monitor.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus I (2+ pts)\n",
    "\n",
    "Implement and train double q-learning.\n",
    "\n",
    "This task contains of\n",
    "* Implementing __double q-learning__ or __dueling q-learning__ or both (see tips below)\n",
    "* Training a network till convergence\n",
    "  * Full points will be awarded if your network gets average score of >=10 (see \"evaluating results\")\n",
    "  * Higher score = more points as usual\n",
    "  * If you're running out of time, it's okay to submit a solution that hasn't converged yet and updating it when it converges. _Lateness penalty will not increase for second submission_, so submitting first one in time gets you no penalty.\n",
    "\n",
    "\n",
    "#### Tips:\n",
    "* Implementing __double q-learning__ shouldn't be a problem if you've already have target networks in place.\n",
    "  * You will probably need `tf.argmax` to select best actions\n",
    "  * Here's an original [article](https://arxiv.org/abs/1509.06461)\n",
    "\n",
    "* __Dueling__ architecture is also quite straightforward if you have standard DQN.\n",
    "  * You will need to change network architecture, namely the q-values layer\n",
    "  * It must now contain two heads: V(s) and A(s,a), both dense layers\n",
    "  * You should then add them up via elemwise sum layer.\n",
    "  * Here's an [article](https://arxiv.org/pdf/1511.06581.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus II (5+ pts): Prioritized experience replay\n",
    "\n",
    "In this section, you're invited to implement prioritized experience replay\n",
    "\n",
    "* You will probably need to provide a custom data structure\n",
    "* Once pool.update is called, collect the pool.experience_replay.observations, actions, rewards and is_alive and store them in your data structure\n",
    "* You can now sample such transitions in proportion to the error (see [article](https://arxiv.org/abs/1511.05952)) for training.\n",
    "\n",
    "It's probably more convenient to explicitly declare inputs for \"sample observations\", \"sample actions\" and so on to plug them into q-learning.\n",
    "\n",
    "Prioritized (and even normal) experience replay should greatly reduce amount of game sessions you need to play in order to achieve good performance. \n",
    "\n",
    "While it's effect on runtime is limited for atari, more complicated envs (further in the course) will certainly benefit for it.\n",
    "\n",
    "Prioritized experience replay only supports off-policy algorithms, so pls enforce `n_steps=1` in your q-learning reference computation (default is 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuroevolution Search method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_initialization(size,n_input,n_output):\n",
    "    return [MLModel(n_input,n_output) for _ in range(size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitness_score(model,agent,env):\n",
    "    agent.model = model\n",
    "    return evaluate(env, agent, n_games=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breed(parent1,parent2,mutation_rate=0.1):\n",
    "    gen1 = parent1.genome\n",
    "    gen2 = parent2.genome\n",
    "    gen_length= gen1.numel()\n",
    "    genes_pool = torch.stack((gen1,gen2),dim=1)\n",
    "    #cross_over\n",
    "    child_genes = genes_pool.gather(1, index=torch.randint(0,2,size=(gen_length,1),dtype=torch.int64))\n",
    "    child_genes.squeeze_()\n",
    "    #mutation\n",
    "    if mutation_rate>0:\n",
    "        mutated_num = int(gen_length * mutation_rate)\n",
    "        mutated_idx = torch.randint(gen_length,(mutated_num,),dtype=torch.int64)\n",
    "        mutation_vals = torch.rand((mutated_num,))-0.5\n",
    "        child_genes.index_add_(0,mutated_idx,mutation_vals)\n",
    "        \n",
    "        \n",
    "    child = MLModel(parent2.input_size,parent2.output_size)\n",
    "    child.update_genome(child_genes)\n",
    "    \n",
    "    return child "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve(populations,fitness_scores,agent,env, survive_rate = 0.2,epsilon = 0.001):\n",
    "    # test fitness\n",
    "    pop_length= len(populations)\n",
    "    \n",
    "    # convert fitness_scores to probablity for survive\n",
    "    scores= np.array(fitness_scores)\n",
    "    min_score = np.min(scores,axis=0)\n",
    "    scores = scores- min_score + epsilon\n",
    "    probablities = (scores/ np.linalg.norm(scores))**2\n",
    "    \n",
    "    #select survivors\n",
    "    survive_count = int(pop_length * survive_rate)\n",
    "    survive_idx = np.random.choice(pop_length,size=survive_count,p = probablities)\n",
    "    survivors = np.array(populations)[survive_idx].tolist()\n",
    "    \n",
    "    #breed\n",
    "    child_count = pop_length - survive_count\n",
    "    children = []\n",
    "    \n",
    "    for _ in range(child_count):\n",
    "        p1,p2 = np.random.choice(survive_count,size=2)\n",
    "        parent1 = survivors[p1]\n",
    "        parent2 = survivors[p2]\n",
    "        child = breed(parent1,parent2)\n",
    "        children.append(child)\n",
    "        \n",
    "    new_pops = survivors + children\n",
    "    fitness_scores = [get_fitness_score(model,agent,env) for model in new_pops]\n",
    "    \n",
    "    return new_pops,fitness_scores\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from collections import deque\n",
    "POP_SIZE=50\n",
    "GENERATION_NUM = 1000\n",
    "print('training...')\n",
    "# initialize polulation\n",
    "pops = population_initialization(POP_SIZE,state_dim,n_actions)\n",
    "agent = DQNAgent(state_dim,n_actions,pops[0],epsilon=0)\n",
    "evolve_env = gym.make(\"LunarLander-v2\")\n",
    "evolve_env.seed(0)\n",
    "score_history1 = deque(maxlen=50)\n",
    "score_history2 = deque(maxlen=50)\n",
    "score_history3 = deque(maxlen=50)\n",
    "\n",
    "fitness_scores = [get_fitness_score(model,agent,env) for model in pops]\n",
    "best_pop = pops[0]\n",
    "\n",
    "for i in range(GENERATION_NUM):\n",
    "    pops, fitness_scores = evolve(pops,fitness_scores,agent,evolve_env)\n",
    "    score_sort_idx = np.argsort(fitness_scores)\n",
    "\n",
    "    score_history1.append(fitness_scores[score_sort_idx[-1]])\n",
    "    score_history2.append(fitness_scores[score_sort_idx[-2]])\n",
    "    score_history3.append(fitness_scores[score_sort_idx[-3]])\n",
    "    print('\\rEpisode {}  Top 3 Scores: {:.2f}, {:.2f}, {:.2f}'.format(i, \\\n",
    "                    np.mean(score_history1),np.mean(score_history2),np.mean(score_history3)),end=\"\")\n",
    "    \n",
    "    if i %100 == 0:\n",
    "        print('\\rEpisode {}  Top 3 Scores: {:.2f}, {:.2f}, {:.2f}'.format(i, \\\n",
    "                    np.mean(score_history1),np.mean(score_history2),np.mean(score_history3)))\n",
    "    \n",
    "    if np.mean(score_history1) >= 200:\n",
    "        print(\"\\nEvolution reaches target species with score:{}\".format(fitness_scores[score_sort_idx[-1]]))\n",
    "        best_pop = pops[score_sort_idx[-1]]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model = best_pop\n",
    "evaluate(evolve_env, agent, n_games=1,render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient Monte Carlo Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "class PGAgent():\n",
    "    def __init__(self,model):\n",
    "        \"\"\"A simple PG agent\"\"\"\n",
    "        self.model = model\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        \"\"\"interface for Agent\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        s = torch.from_numpy(np.expand_dims(state,axis=0)).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model.forward(s)\n",
    "            #m = Categorical(logits = logits)\n",
    "            #action = m.sample()\n",
    "            # return action.cpu().numpy().data[0]\n",
    "            probs = F.softmax(logits,dim=1).squeeze().cpu().numpy()\n",
    "        return np.random.choice(len(probs),p=probs)\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Transition = namedtuple('Transition',('state', 'action', 'reward','done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run step and reset for gym batch environment\n",
    "def run_step(env,a):\n",
    "    s,r,done, _ = env.step(a)\n",
    "    if done:\n",
    "        s = env.reset()\n",
    "    return Transition(s,a,r,done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-55af2b515307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \"\"\"\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfigure_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mshow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[1;32m   2210\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m                     \u001b[0mdryrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2212\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2213\u001b[0m                 \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cachedRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m                 \u001b[0mbbox_inches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0moriginal_dpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;31m# if toolbar:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m#     toolbar.set_cursor(cursors.WAIT)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1475\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2605\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2607\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         ticklabelBoxes, ticklabelBoxes2 = self._get_tick_bboxes(ticks_to_draw,\n\u001b[1;32m   1192\u001b[0m                                                                 renderer)\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_ticks\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0minterval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_view_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m         \u001b[0mtick_tups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# iter_ticks calls the locator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_bounds\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtick_tups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;31m# handle inverted limits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36miter_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0mmajorLocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0mmajorTicks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_major_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajorLocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_locs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajorLocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m         majorLabels = [self.major.formatter(val, i)\n\u001b[1;32m    975\u001b[0m                        for i, val in enumerate(majorLocs)]\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36mset_locs\u001b[0;34m(self, locs)\u001b[0m\n\u001b[1;32m    664\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_orderOfMagnitude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36m_set_format\u001b[0;34m(self, vmin, vmax)\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0m_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_locs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10.\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderOfMagnitude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         \u001b[0mloc_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;31m# Curvilinear coordinates can yield two identical points.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloc_range\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mptp\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   2217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m     \"\"\"\n\u001b[0;32m-> 2219\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ptp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from torch.distributions.categorical import Categorical\n",
    "# for training\n",
    "STEPS = 100000\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "batch_envs = [gym.make(\"LunarLander-v2\") for _ in range(BATCH_SIZE)]\n",
    "# for evaluation\n",
    "eval_env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_dim, n_actions = eval_env.observation_space.shape[0], eval_env.action_space.n\n",
    "\n",
    "actor_model = MLModel(state_dim, n_actions, n_l1=64, n_l2=256).to(device)\n",
    "critic_model = MLModel(state_dim, 1, n_l1=64, n_l2=128).to(device)\n",
    "agent = PGAgent(actor_model)\n",
    "\n",
    "\n",
    "optimizer_actor = torch.optim.Adam(actor_model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_critic = torch.optim.Adam(critic_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "eval_env.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "t_max = eval_env.spec.timestep_limit or 1000\n",
    "#t_max = 200\n",
    "\n",
    "mean_rw_history = []\n",
    "loss_history = []\n",
    "entropy_history =[]\n",
    "baseline_history = []\n",
    "critic_loss_history = []\n",
    "steps_history = []\n",
    "\n",
    "states = [env.reset() for env in batch_envs]\n",
    "states_t = torch.FloatTensor(states).to(device)\n",
    "for i in range(STEPS):\n",
    "    \n",
    "    logits_t = actor_model(states_t)\n",
    "    m = Categorical(logits = logits_t)\n",
    "    actions_t = m.sample()\n",
    "    \n",
    "    batch_transitions = [run_step(env,a) for env, a in zip(batch_envs,actions_t.cpu().data.numpy())] \n",
    "    batch = Transition(*zip(*batch_transitions))\n",
    "    \n",
    "    next_states_t = torch.FloatTensor(batch.state).to(device)\n",
    "    rewards_t = torch.FloatTensor(batch.reward).to(device)\n",
    "    done_t = torch.FloatTensor(batch.done).to(device)\n",
    "    \n",
    "    # critic loss\n",
    "    predicted_states_v = critic_model(states_t).squeeze()\n",
    "    with torch.no_grad():\n",
    "        predicted_next_states_v = critic_model(next_states_t).squeeze()*(1-done_t)\n",
    "        target_states_v = predicted_next_states_v + rewards_t\n",
    "        \n",
    "    L_critic = F.smooth_l1_loss(predicted_states_v, target_states_v)\n",
    "    \n",
    "    optimizer_critic.zero_grad()\n",
    "    L_critic.backward()\n",
    "    optimizer_critic.step()\n",
    "    \n",
    "    \n",
    "     # actor loss\n",
    "    log_probs_t = m.log_prob(actions_t) # here is graph construct\n",
    "    advantages_t = (target_states_v - predicted_states_v).detach()\n",
    "    J_actor = (advantages_t * log_probs_t).mean()\n",
    "\n",
    "    # entropy\n",
    "    entropy = m.entropy().mean()\n",
    "\n",
    "    L_actor = -J_actor - entropy * 0.1\n",
    "    optimizer_actor.zero_grad()\n",
    "    L_actor.backward()\n",
    "\n",
    "    states_t = next_states_t\n",
    "    \n",
    "    loss_history.append(L_actor.cpu().data.numpy())\n",
    "    critic_loss_history.append(L_critic.cpu().data.numpy())\n",
    "    entropy_history.append(entropy.cpu().data.numpy())\n",
    "    baseline_history.append(predicted_states_v.mean().cpu().data.numpy())\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        \n",
    "        mean_rw_history.append(evaluate(eval_env, agent, n_games=5))\n",
    "        \n",
    "        if mean_rw_history[-1] > 200:\n",
    "            print(\"Reach the target score 200 of 5 games\")\n",
    "            break\n",
    "        \n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=[16, 8])        \n",
    "        plt.subplot(2,3,1)\n",
    "        plt.title(\"mean reward per game\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.subplot(2,3,2)\n",
    "        plt.title(\"Avg V\")\n",
    "        plt.plot(baseline_history)\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.subplot(2,3,3)\n",
    "        plt.title(\"Entropy\")\n",
    "        plt.plot(entropy_history)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2,3,4)\n",
    "        plt.title(\"Actor Loss\")\n",
    "        plt.plot(loss_history)\n",
    "        plt.grid()\n",
    "    \n",
    "        \n",
    "        plt.subplot(2,3,5)\n",
    "        plt.title(\"Critic Loss\")\n",
    "        plt.plot(critic_loss_history)\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.show()\n",
    "          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(eval_env, agent, n_games=1,render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
