{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network implementation\n",
    "\n",
    "This notebook shamelessly demands you to implement a DQN - an approximate q-learning algorithm with experience replay and target networks - and see if it works any better this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00359449,  1.4195306 , -0.36410254,  0.38268712,  0.00417193,\n",
       "        0.08247469,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.render(mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4bf5bbc7f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img[:320,200:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(self, state_shape, n_actions,model, epsilon=0):\n",
    "        \"\"\"A simple DQN agent\"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape= state_shape\n",
    "        self.model = model\n",
    "\n",
    "    def get_qvalues(self, state):\n",
    "        \"\"\"\n",
    "        state is observation from gym environment\n",
    "        pytorch needs [batch,s], so add a batch dim in state\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        s = torch.from_numpy(np.expand_dims(state,axis=0))\n",
    "        with torch.no_grad():\n",
    "            q = self.model.forward(s)\n",
    "        return np.squeeze(q.data.numpy(),axis=0)\n",
    "    \n",
    "    def sample_action(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        \n",
    "        p = np.ones(self.n_actions,dtype=np.float) * self.epsilon/self.n_actions\n",
    "        p[qvalues.argmax(axis=-1)] += 1-self.epsilon\n",
    "        action = np.random.choice(n_actions, p=p)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network\n",
    "\n",
    "We now need to build a neural network that can map images to state q-values. This network will be called on every agent's step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLModel(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(MLModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size,32)\n",
    "        self.layer2 = nn.Linear(32,64)\n",
    "        self.output = nn.Linear(64,output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_dim,n_actions = env.observation_space.shape[0],env.action_space.n\n",
    "linearModel = MLModel(state_dim,n_actions).to(device)\n",
    "agent = DQNAgent(state_dim, n_actions,linearModel, epsilon=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct train & evaluation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our agent to see if it raises any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    t_max = env.spec.timestep_limit or 10000\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues(s)\n",
    "            action = agent.sample_action(qvalues)\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done: break\n",
    "                \n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-310.00094451529384"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(env, agent, n_games=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "#### The interface:\n",
    "* `exp_replay.push(state, action, reward, next_state, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward','next_state','done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def head(self):\n",
    "        return self.memory[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    \n",
    "    :returns: return sum of rewards over time\n",
    "    \"\"\"\n",
    "    # initial state\n",
    "    done = False\n",
    "    s = env.reset()\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    for i in range(n_steps):\n",
    "        qvalues = agent.get_qvalues(s)\n",
    "        a = agent.sample_action(qvalues)\n",
    "        next_s,r,done,_ = env.step(a)\n",
    "        exp_replay.push(s,a,r,next_s,done)\n",
    "        s = next_s\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_replay = ReplayMemory(10000)\n",
    "play_and_record(agent,env,exp_replay,n_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = MLModel(state_dim,n_actions).to(device)\n",
    "target_net = MLModel(state_dim,n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "agent.model = policy_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with... Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{policy_\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "With Q-reference defined as\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Where\n",
    "* $Q_{target}(s',a')$ denotes q-value of next state and next action predicted by __target_network__\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above.\n",
    "\n",
    "\n",
    "__Note 1:__ there's an example input below. Feel free to experiment with it before you write the function.\n",
    "__Note 2:__ compute_td_loss is a source of 99% of bugs in this homework. If reward doesn't improve, it often helps to go through it line by line [with a rubber duck](https://rubberduckdebugging.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = exp_replay.sample(100)\n",
    "# Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "# detailed explanation).\n",
    "batch = Transition(*zip(*transitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_loss(target_net,policy_net,states,actions,rewards,next_states,is_dones,GAMMA = 0.99):\n",
    "    s_ = torch.as_tensor(states)\n",
    "    a_ = torch.as_tensor(actions)\n",
    "    r_ = torch.as_tensor(rewards)\n",
    "    ns_= torch.as_tensor(next_states)\n",
    "    d_= torch.as_tensor(is_dones)\n",
    "    not_d_ = 1 - d_\n",
    "    \n",
    "    #predict Q values\n",
    "    predict_q_ = policy_net(s_).gather(dim=1,index=a_.view(-1,1)).squeeze()\n",
    "    \n",
    "    #predict Next QValues with target network\n",
    "    with torch.no_grad():\n",
    "        predict_next_q_ = target_net(s_)\n",
    "        predict_v_ = torch.max(predict_next_q_,dim=1)[0] * not_d_\n",
    "        reference_q_ = r_ + GAMMA * predict_v_\n",
    "    \n",
    "    loss = F.mse_loss(predict_q_,reference_q_)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd_states= np.array(batch.state,dtype=np.float32)\n",
    "nd_actions= np.array(batch.action,dtype=np.int64)\n",
    "nd_rewards = np.array(batch.reward,dtype=np.float32)\n",
    "nd_next_states = np.array(batch.next_state,dtype=np.float32)\n",
    "nd_is_dones = np.array(batch.done).astype(np.float32)\n",
    "\n",
    "loss = td_loss(target_net,policy_net,nd_states,nd_actions,nd_rewards,nd_next_states,nd_is_dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop\n",
    "\n",
    "It's time to put everything together and see if it learns anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=100\n",
    "TIME_STEPS=5*10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_net = MLModel(state_dim,n_actions).to(device)\n",
    "target_net = MLModel(state_dim,n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "agent.model = policy_net\n",
    "opt = torch.optim.Adam(policy_net.parameters())\n",
    "exp_replay = ReplayMemory(10**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "s = env.reset()\n",
    "for i in trange(TIME_STEPS):\n",
    "    \n",
    "    # play\n",
    "    q_values = agent.get_qvalues(s)\n",
    "    a = agent.sample_action(qvalues)\n",
    "    s_next,r,done, _ = env.step(a)\n",
    "    \n",
    "    exp_replay.push(s,a,r,s_next,done)\n",
    "    \n",
    "    s = s_next\n",
    "    \n",
    "    if done:\n",
    "        s = env.reset()\n",
    "    \n",
    "    if len(exp_replay) < BATCH_SIZE:\n",
    "        continue\n",
    "    \n",
    "    # train\n",
    "    < sample data from experience replay>\n",
    "    \n",
    "    loss = < compute TD loss >\n",
    "    \n",
    "    < minimize loss by gradient descent >\n",
    "    \n",
    "    td_loss_history.append(loss.data.cpu().numpy()[0])\n",
    "    \n",
    "    # adjust agent parameters\n",
    "    if i % 500 == 0:\n",
    "        agent.epsilon = max(agent.epsilon * 0.99, 0.01)\n",
    "        mean_rw_history.append(evaluate(make_env(), agent, n_games=3))\n",
    "        \n",
    "        #Load agent weights into target_network\n",
    "        <YOUR CODE>\n",
    "        \n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "        plt.figure(figsize=[12, 4])        \n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title(\"mean reward per game\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        assert not np.isnan(td_loss_history[-1])\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title(\"TD loss history (moving average)\")\n",
    "        plt.plot(pd.ewma(np.array(td_loss_history), span=100, min_periods=100))\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert np.mean(mean_rw_history[-10:]) > 10.\n",
    "print(\"That's good enough for tutorial.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ How to interpret plots: __\n",
    "\n",
    "\n",
    "This aint no supervised learning so don't expect anything to improve monotonously. \n",
    "* __ TD loss __ is the MSE between agent's current Q-values and target Q-values. It may slowly increase or decrease, it's ok. The \"not ok\" behavior includes going NaN or stayng at exactly zero before agent has perfect performance.\n",
    "* __ mean reward__ is the expected sum of r(s,a) agent gets over the full game session. It will oscillate, but on average it should get higher over time (after a few thousand iterations...). \n",
    " * In basic q-learning implementation it takes 5-10k steps to \"warm up\" agent before it starts to get better.\n",
    "* __ buffer size__ - this one is simple. It should go up and cap at max size.\n",
    "* __ epsilon__ - agent's willingness to explore. If you see that agent's already at 0.01 epsilon before it's average reward is above 0 - __ it means you need to increase epsilon__. Set it back to some 0.2 - 0.5 and decrease the pace at which it goes down.\n",
    "* Also please ignore first 100-200 steps of each plot - they're just oscillations because of the way moving average works.\n",
    "\n",
    "At first your agent will lose quickly. Then it will learn to suck less and at least hit the ball a few times before it loses. Finally it will learn to actually score points.\n",
    "\n",
    "__Training will take time.__ A lot of it actually. An optimistic estimate is to say it's gonna start winning (average reward > 10) after 20k steps. \n",
    "\n",
    "But hey, long training time isn't _that_ bad:\n",
    "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/training.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.epsilon=0 # Don't forget to reset epsilon back to previous value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env_monitor = gym.wrappers.Monitor(make_env(),directory=\"videos\",force=True)\n",
    "sessions = [evaluate(env_monitor, agent, n_games=1) for _ in range(100)]\n",
    "env_monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment part I (5 pts)\n",
    "\n",
    "We'll start by implementing target network to stabilize training.\n",
    "\n",
    "To do that you should use TensorFlow functionality. \n",
    "\n",
    "We recommend thoroughly debugging your code on simple tests before applying it in atari dqn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus I (2+ pts)\n",
    "\n",
    "Implement and train double q-learning.\n",
    "\n",
    "This task contains of\n",
    "* Implementing __double q-learning__ or __dueling q-learning__ or both (see tips below)\n",
    "* Training a network till convergence\n",
    "  * Full points will be awarded if your network gets average score of >=10 (see \"evaluating results\")\n",
    "  * Higher score = more points as usual\n",
    "  * If you're running out of time, it's okay to submit a solution that hasn't converged yet and updating it when it converges. _Lateness penalty will not increase for second submission_, so submitting first one in time gets you no penalty.\n",
    "\n",
    "\n",
    "#### Tips:\n",
    "* Implementing __double q-learning__ shouldn't be a problem if you've already have target networks in place.\n",
    "  * You will probably need `tf.argmax` to select best actions\n",
    "  * Here's an original [article](https://arxiv.org/abs/1509.06461)\n",
    "\n",
    "* __Dueling__ architecture is also quite straightforward if you have standard DQN.\n",
    "  * You will need to change network architecture, namely the q-values layer\n",
    "  * It must now contain two heads: V(s) and A(s,a), both dense layers\n",
    "  * You should then add them up via elemwise sum layer.\n",
    "  * Here's an [article](https://arxiv.org/pdf/1511.06581.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus II (5+ pts): Prioritized experience replay\n",
    "\n",
    "In this section, you're invited to implement prioritized experience replay\n",
    "\n",
    "* You will probably need to provide a custom data structure\n",
    "* Once pool.update is called, collect the pool.experience_replay.observations, actions, rewards and is_alive and store them in your data structure\n",
    "* You can now sample such transitions in proportion to the error (see [article](https://arxiv.org/abs/1511.05952)) for training.\n",
    "\n",
    "It's probably more convenient to explicitly declare inputs for \"sample observations\", \"sample actions\" and so on to plug them into q-learning.\n",
    "\n",
    "Prioritized (and even normal) experience replay should greatly reduce amount of game sessions you need to play in order to achieve good performance. \n",
    "\n",
    "While it's effect on runtime is limited for atari, more complicated envs (further in the course) will certainly benefit for it.\n",
    "\n",
    "Prioritized experience replay only supports off-policy algorithms, so pls enforce `n_steps=1` in your q-learning reference computation (default is 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
